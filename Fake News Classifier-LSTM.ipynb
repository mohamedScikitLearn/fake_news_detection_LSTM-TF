{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook will guide throw the process of detecting Fake news on Social media using Machine learning algorithms and Deep LSTM using Tensorflow"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/fake-news/submit.csv\n/kaggle/input/fake-news/test.csv\n/kaggle/input/fake-news/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/fake-news/train.csv')\ntest=pd.read_csv('/kaggle/input/fake-news/test.csv')\nsubmit=pd.read_csv('/kaggle/input/fake-news/submit.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=train.dropna() # Removing every missing value existing on the dataframe ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df['title'] # Make Text column in Variable X\ny=df['label'] # Make the labels on variable Y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the features ‘title’, ‘author’ and ‘text’ are important and all are in text form. So, we can combine these features to make one final feature which we will use to train the model. Let’s call the feature ‘total’.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Firstly, fill all the null spaces with a space\ntrain = train.fillna(' ')\ntrain['total'] = train['title'] + ' ' + train['author'] + ' ' +  train['text']\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install nltk ","execution_count":9,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.14.0)\n\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Machine learning experiments "},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaning our dataset \nimport nltk \nfrom nltk.corpus import stopwords # to remove stop words such as ' the , they , it, a ...'\nfrom nltk.stem import WordNetLemmatizer # for lemmatization task \n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english')\n","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenization: Word tokenization is the process of splitting a large sample of text into words.\nFor example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nword_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\nnltk_tokens = nltk.word_tokenize(word_data)\nprint(nltk_tokens)\n","execution_count":13,"outputs":[{"output_type":"stream","text":"['It', 'originated', 'from', 'the', 'idea', 'that', 'there', 'are', 'readers', 'who', 'prefer', 'learning', 'new', 'skills', 'from', 'the', 'comforts', 'of', 'their', 'drawing', 'rooms']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfor index, row in train.iterrows():\n    filter_sentence = ''\n    sentence = row['total']\n    # Cleaning the sentence with regex\n    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n    # Tokenization\n    words = nltk.word_tokenize(sentence)\n    # Stopwords removal\n    words = [w for w in words if not w in stop_words]\n    # Lemmatization\n    for words in words:\n        filter_sentence = filter_sentence  + ' ' +  str(lemmatizer.lemmatize(words)).lower()\n    train.loc[index, 'total'] = filter_sentence\ntrain = train[['total', 'label']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train['total']\nY_train = train['label']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vectorizer**\n\nFor converting this text data into numerical data, we will use two vectorizers.\n* Count Vectorizer\n\nIn order to use textual data for predictive modelling, the text must be parsed to remove certain words — this process is called tokenization. These words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorization).\n\n* **TF-IDF Vectorizer**\nTF-IDF stands for Term Frequency — Inverse Document Frequency. It is one of the most important techniques used for information retrieval to represent how important a specific word or phrase is to a given document.\nRead more about this here."},{"metadata":{"trusted":true},"cell_type":"code","source":"# this could take a while.\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncount_vectorizer = CountVectorizer()\ncount_vectorizer.fit_transform(X_train)\nfreq_term_matrix = count_vectorizer.transform(X_train)\ntfidf = TfidfTransformer(norm = \"l2\")\ntfidf.fit(freq_term_matrix)\ntf_idf_matrix = tfidf.fit_transform(freq_term_matrix)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Models "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(tf_idf_matrix,  Y_train, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nAccuracy = logreg.score(X_test, y_test)\nprint( 'LogisticRegression Accuracy :  ',Accuracy )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Multinomial Naive Bayes Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nNB = MultinomialNB()\nNB.fit(X_train, y_train)\nAccuracy = NB.score(X_test, Y_test)\nprint( 'Multinomial NB Accuracy :  ',Accuracy )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep learning experiments "},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf # Import Latest tensorflow version \ntf.__version__","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"'2.1.0'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, LSTM, Dense ## Neural networks layers \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot # to encode the depending variable \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voc_size=5000 # max num words to take into consideration while training your model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=[i.lower() for i in X] # lowercase each text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot=[one_hot(words,voc_size) for words in X] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sen_len=30\nembedded_doc=pad_sequences(onehot, padding='pre', maxlen=sen_len) # pad sequence your texts\nprint(embedded_doc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_vector_feature=40 \nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_feature, input_length=sen_len))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\n# sigmoid : to handle the output ( binary case )\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# binary_crossentropy : because we have a binaray classification task \n# Adam : Stochastic gradient decenet optimizatiion \nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final=np.array(embedded_doc)\ny_final=np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X_final, y_final, test_size=0.20, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://medium.com/@ishantjuyal/fake-news-detector-nlp-project-9d67e0177075\nand many other Kaggle Kernels "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}